{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqR_RON5SK0P"
      },
      "source": [
        "# ***Text Summarization***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP5EULYCVKPj"
      },
      "source": [
        "# ***Using Basic Algo***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "20TJpWV6RJGz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.50.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\loq\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\loq\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\loq\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.29.3)\n",
            "Requirement already satisfied: Pillow in c:\\users\\loq\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\loq\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\loq\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\loq\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\loq\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\loq\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\loq\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\loq\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
            "Downloading sentence_transformers-4.0.2-py3-none-any.whl (340 kB)\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-4.0.2\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This framework provides an easy method to compute dense vector representations for sentences, paragraphs, and images. The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. and achieve state-of-the-art performance in various tasks. Text is embedded in vector space such that similar text are closer and can efficiently be found using cosine similarity.\n",
        "\"\"\"\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsWY78NrRMX-",
        "outputId": "aa175b74-ca64-42df-9e5c-a753ab2aa486"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\LOQ\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\LOQ\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# importing the required modules\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iE0Gum7RMQY",
        "outputId": "04b51863-3c70-44fd-b371-c99249404a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary: machines integral part human progress centuries significantly transformed industries economies daily life essay explore definition evolution machines impact society significant contributors field various perspectives implications machine development future developments likely shape modern landscape firstly machines defined mechanical devices utilize energy perform tasks vary simple tools like levers pulleys complex systems like computers robots throughout history machines evolved rudimentary designs sophisticated entities industrial revolution marked pivotal moment machine development introduction steam power revolutionized industries leading mass production efficiency one discuss evolution machines without recognizing key figures contributed advancement instance james watt wellknown improving steam engine played crucial role industrial revolution innovations sparked wave mechanization changing way people worked similarly nikola tesla thomas edison instrumental development electricity electrical machinery inventions laid groundwork modern machines automation prevalent today recent years figures like elon musk pioneers artificial intelligence emerged significant influencers realm machine development work continues push boundaries machines achieve impact machines society profound one hand machines increased productivity improved efficiency reduced manual labor instance agricultural machines transformed farming leading increased crop yields food production similarly manufacturing machines enabled higher production rates reducing costs enhancing accessibility advancements contribute economic growth improvement living standards however rise machines also led significant societal challenges one pressing concerns impact automation employment machines taking repetitive tasks many fear job displacement concern echoed various sectors including manufacturing clerical work even professional services world economic forum predicts automation may displace millions jobs creating new ones require different skill sets transition lead social upheaval calls policies address workforce retraining education ethical perspective reliance machines raises questions autonomy privacy security machines become intelligent decisionmaking capabilities garnered significant attention development artificial intelligence particularly machine learning led machines learn data make decisions without human intervention raises ethical dilemmas regarding accountability transparency automated vehicle causes accident responsible questions challenge notion free human oversight critical decisions moreover integration machines daily life presents psychological impacts dependency technology lead decreased physical activity social interaction example prevalence personal devices like smartphones altered communication patterns sometimes fostering isolation rather connection therefore machines enhance convenience efficiency lead unintended consequences individual behavior societal norms despite challenges future machines promising innovations like robotics artificial intelligence internet things pushing machines operate unprecedented levels industries increasingly integrating smart machines communicate share data optimize performance interconnectedness paving way digital economy machines make realtime decisions improving efficiency across sectors furthermore environmental concerns gain prominence development machines aimed sustainability rise green technologies energyefficient machines renewable energy systems represent significant shift towards minimizing ecological footprint innovations demonstrate adaptability machine technology also potential address pressing global challenges like climate change conclusion machines fundamentally altered work live interact world offer remarkable benefits increased productivity enhanced quality life also present significant ethical social economic challenges ongoing dialogue implications machine development essential navigating complex landscape look future integration machines must balanced considerations human welfare societal impact ethical responsibility discourse surrounding machines continue evolve technological advancements reshape understanding interaction powerful tools references 1 j watt improvements steam engine proceedings royal society 1776 pp 2325 2 n tesla alternating current electricity electrical engineering vol 12 4 pp 5660 1893 3 edison electric light journal electrical engineering vol 10 1 pp 112 1879 4 world economic forum future jobs report 2020 5 j doe ethics ai autonomous machines ai ethics journal vol 34 2 pp 101115 2022 6 national renewable energy laboratory green technologies sustainable machines 2021\n"
          ]
        }
      ],
      "source": [
        "def preprocess(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenization\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Join words to form the cleaned-up text\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def summarize(text, n):\n",
        "    sentences = text.split('.')\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
        "    scores = np.sum(cosine_similarity(sentence_vectors[0:1], sentence_vectors), axis=1)\n",
        "    top_sentence_indices = np.argsort(scores)[::-1][:n]\n",
        "    summary = '. '.join(np.array(sentences)[top_sentence_indices])\n",
        "    return summary\n",
        "\n",
        "document = \"\"\"Machines have been an integral part of human progress for centuries. They have significantly transformed industries, economies, and daily life. In this essay, we will explore the definition and evolution of machines, their impact on society, significant contributors to the field, various perspectives on the implications of machine development, and future developments that are likely to shape the modern landscape.\n",
        "\n",
        "Firstly, machines can be defined as mechanical devices that utilize energy to perform tasks. They vary from simple tools like levers and pulleys to complex systems like computers and robots. Throughout history, machines have evolved from rudimentary designs to sophisticated entities. The Industrial Revolution marked a pivotal moment in machine development. The introduction of steam power revolutionized industries, leading to mass production and efficiency.\n",
        "\n",
        "\n",
        "One cannot discuss the evolution of machines without recognizing key figures who have contributed to their advancement. For instance, James Watt is well-known for improving the steam engine, which played a crucial role during the Industrial Revolution. His innovations sparked a wave of mechanization, changing the way people worked. Similarly, Nikola Tesla and Thomas Edison were instrumental in the development of electricity and electrical machinery. Their inventions laid the groundwork for modern machines and automation that are prevalent today. In recent years, figures like Elon Musk and pioneers in artificial intelligence have emerged as significant influencers in the realm of machine development. Their work continues to push the boundaries of what machines can achieve.\n",
        "\n",
        "The impact of machines on society is profound. On one hand, machines have increased productivity, improved efficiency, and reduced manual labor. For instance, agricultural machines have transformed farming, leading to increased crop yields and food production. Similarly, manufacturing machines have enabled higher production rates, reducing costs and enhancing accessibility. These advancements contribute to economic growth and improvement in living standards.\n",
        "\n",
        "However, the rise of machines has also led to significant societal challenges. One of the most pressing concerns is the impact of automation on employment. With machines taking over repetitive tasks, many fear job displacement. This concern has been echoed in various sectors, including manufacturing, clerical work, and even professional services. The World Economic Forum predicts that automation may displace millions of jobs while creating new ones that require different skill sets. This transition can lead to social upheaval and calls for policies that can address workforce retraining and education.\n",
        "\n",
        "From an ethical perspective, the reliance on machines raises questions about autonomy, privacy, and security. As machines become more intelligent, their decision-making capabilities have garnered significant attention. The development of artificial intelligence, particularly machine learning, has led to machines that can learn from data and make decisions without human intervention. This raises ethical dilemmas regarding accountability and transparency. If an automated vehicle causes an accident, who is responsible? Such questions challenge the very notion of free will and human oversight in critical decisions.\n",
        "\n",
        "Moreover, the integration of machines into daily life presents psychological impacts. The dependency on technology can lead to decreased physical activity and social interaction. For example, the prevalence of personal devices like smartphones has altered communication patterns, sometimes fostering isolation rather than connection. Therefore, while machines enhance convenience and efficiency, they can lead to unintended consequences on individual behavior and societal norms.\n",
        "\n",
        "Despite these challenges, the future of machines is promising. Innovations like robotics, artificial intelligence, and the Internet of Things are pushing machines to operate at unprecedented levels. Industries are increasingly integrating smart machines that can communicate, share data, and optimize performance. This interconnectedness is paving the way for a digital economy where machines can make real-time decisions, improving efficiency across sectors.\n",
        "\n",
        "Furthermore, as environmental concerns gain prominence, the development of machines aimed at sustainability is on the rise. Green technologies, energy-efficient machines, and renewable energy systems represent a significant shift towards minimizing the ecological footprint. These innovations not only demonstrate the adaptability of machine technology but also its potential to address pressing global challenges like climate change.\n",
        "\n",
        "In conclusion, machines have fundamentally altered how we work, live, and interact with the world. While they offer remarkable benefits, such as increased productivity and enhanced quality of life, they also present significant ethical, social, and economic challenges. The ongoing dialogue about the implications of machine development is essential in navigating this complex landscape. As we look to the future, the integration of machines must be balanced with considerations of human welfare, societal impact, and ethical responsibility. The discourse surrounding machines will continue to evolve as technological advancements reshape our understanding and interaction with these powerful tools.\n",
        "\n",
        "References\n",
        "\n",
        "[1] J. Watt, \"Improvements on the steam engine,\" in Proceedings of the Royal Society, 1776, pp. 23-25.\n",
        "\n",
        "[2] N. Tesla, \"Alternating Current Electricity,\" in Electrical Engineering, vol. 12, no. 4, pp. 56-60, 1893.\n",
        "\n",
        "[3] T. Edison, \"Electric Light,\" in Journal of Electrical Engineering, vol. 10, no. 1, pp. 1-12, 1879.\n",
        "\n",
        "[4] World Economic Forum, \"The Future of Jobs Report,\" 2020.\n",
        "\n",
        "[5] J. Doe, \"Ethics of AI and Autonomous Machines,\" in AI Ethics Journal, vol. 34, no. 2, pp. 101-115, 2022.\n",
        "\n",
        "[6] National Renewable Energy Laboratory, \"Green Technologies and Sustainable Machines,\" 2021.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "summary = summarize(preprocess(document), 4)\n",
        "print(\"Summary:\", summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl7EOHmZUERk",
        "outputId": "a46c2cd2-96db-4965-cfaf-ca26554bd2ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in c:\\users\\loq\\anaconda3\\lib\\site-packages (from rouge) (1.16.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP6BfSgETuh0",
        "outputId": "5d7823c9-c5bb-45fb-b77a-7bdf3c01e336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE Scores: {'rouge-1': {'r': 0.5906313645621182, 'p': 0.7651715039577837, 'f': 0.666666661749531}, 'rouge-2': {'r': 0.19695044472681067, 'p': 0.29245283018867924, 'f': 0.23538344241894885}, 'rouge-l': {'r': 0.5906313645621182, 'p': 0.7651715039577837, 'f': 0.666666661749531}}\n",
            "Original Text Word Count: 1013\n",
            "Generated Summary Word Count: 550\n",
            "Compression Ratio (Summary to Original): 0.5429417571569596\n",
            "Relative Length to Reference: 0.5429417571569596\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from rouge import Rouge\n",
        "\n",
        "def evaluate_summary(generated_summary, reference_summary, original_text):\n",
        "    # Calculate ROUGE scores\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(summary, document, avg=True)\n",
        "\n",
        "    # Word count metrics\n",
        "    original_word_count = len(word_tokenize(original_text))\n",
        "    summary_word_count = len(word_tokenize(generated_summary))\n",
        "    doc_word_count = len(word_tokenize(reference_summary))\n",
        "\n",
        "    # Calculate word count ratios\n",
        "    compression_ratio = summary_word_count / original_word_count\n",
        "    relative_length_to_reference = summary_word_count / doc_word_count\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(\"ROUGE Scores:\", scores)\n",
        "    print(\"Original Text Word Count:\", original_word_count)\n",
        "    print(\"Generated Summary Word Count:\", summary_word_count)\n",
        "    print(\"Compression Ratio (Summary to Original):\", compression_ratio)\n",
        "    print(\"Relative Length to Reference:\", relative_length_to_reference)\n",
        "\n",
        "\n",
        "evaluate_summary(summary, document, document)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **BASIC ALGORITHM TEST PDF FILES FOR SUMMARIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rouge import Rouge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[59], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m preprocessed_sentences \u001b[38;5;241m=\u001b[39m preprocess_text(pdf_text)\n\u001b[0;32m     58\u001b[0m summary \u001b[38;5;241m=\u001b[39m summarize_text(preprocessed_sentences)\n\u001b[1;32m---> 59\u001b[0m scores, original_count, summary_count \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUGE Scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, scores)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Word Count:\u001b[39m\u001b[38;5;124m\"\u001b[39m, original_count)\n",
            "Cell \u001b[1;32mIn[59], line 51\u001b[0m, in \u001b[0;36mevaluate_summary\u001b[1;34m(original_text, summary)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_summary\u001b[39m(original_text, summary):\n\u001b[0;32m     50\u001b[0m     rouge \u001b[38;5;241m=\u001b[39m Rouge()\n\u001b[1;32m---> 51\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mrouge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     original_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_tokenize(original_text))\n\u001b[0;32m     53\u001b[0m     summary_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_tokenize(summary))\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge.py:107\u001b[0m, in \u001b[0;36mRouge.get_scores\u001b[1;34m(self, hyps, refs, avg, ignore_empty)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(hyps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(refs))\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m avg:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_avg_scores(hyps, refs)\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge.py:120\u001b[0m, in \u001b[0;36mRouge._get_scores\u001b[1;34m(self, hyps, refs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n\u001b[0;32m    119\u001b[0m     fn \u001b[38;5;241m=\u001b[39m Rouge\u001b[38;5;241m.\u001b[39mAVAILABLE_METRICS[m]\n\u001b[1;32m--> 120\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclusive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclusive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     sen_score[m] \u001b[38;5;241m=\u001b[39m {s: sc[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats}\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_lengths:\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge.py:59\u001b[0m, in \u001b[0;36mRouge.<lambda>\u001b[1;34m(hyp, ref, **k)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mRouge\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     DEFAULT_METRICS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-l\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     52\u001b[0m     AVAILABLE_METRICS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-3\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-4\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-5\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-l\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk:\n\u001b[1;32m---> 59\u001b[0m             \u001b[43mrouge_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrouge_l_summary_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     60\u001b[0m     }\n\u001b[0;32m     61\u001b[0m     DEFAULT_STATS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     62\u001b[0m     AVAILABLE_STATS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:389\u001b[0m, in \u001b[0;36mrouge_l_summary_level\u001b[1;34m(evaluated_sentences, reference_sentences, raw_results, exclusive, **_)\u001b[0m\n\u001b[0;32m    387\u001b[0m union \u001b[38;5;241m=\u001b[39m Ngrams(exclusive\u001b[38;5;241m=\u001b[39mexclusive)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref_s \u001b[38;5;129;01min\u001b[39;00m reference_sentences:\n\u001b[1;32m--> 389\u001b[0m     lcs_count, union \u001b[38;5;241m=\u001b[39m \u001b[43m_union_lcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluated_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mref_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mprev_union\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mexclusive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclusive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m     union_lcs_sum_across_all_references \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lcs_count\n\u001b[0;32m    395\u001b[0m llcs \u001b[38;5;241m=\u001b[39m union_lcs_sum_across_all_references\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:333\u001b[0m, in \u001b[0;36m_union_lcs\u001b[1;34m(evaluated_sentences, reference_sentence, prev_union, exclusive)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eval_s \u001b[38;5;129;01min\u001b[39;00m evaluated_sentences:\n\u001b[0;32m    332\u001b[0m     evaluated_words \u001b[38;5;241m=\u001b[39m _split_into_words([eval_s])\n\u001b[1;32m--> 333\u001b[0m     lcs \u001b[38;5;241m=\u001b[39m \u001b[43m_recon_lcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluated_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclusive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclusive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m     combined_lcs_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lcs)\n\u001b[0;32m    335\u001b[0m     lcs_union \u001b[38;5;241m=\u001b[39m lcs_union\u001b[38;5;241m.\u001b[39munion(lcs)\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:190\u001b[0m, in \u001b[0;36m_recon_lcs\u001b[1;34m(x, y, exclusive)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i, j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 190\u001b[0m recon_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m], \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Ngrams(recon_list, exclusive\u001b[38;5;241m=\u001b[39mexclusive)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m recon_tuple\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:188\u001b[0m, in \u001b[0;36m_recon_lcs.<locals>._recon\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:188\u001b[0m, in \u001b[0;36m_recon_lcs.<locals>._recon\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "    \u001b[1;31m[... skipping similar frames: _recon_lcs.<locals>._recon at line 188 (459 times)]\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:188\u001b[0m, in \u001b[0;36m_recon_lcs.<locals>._recon\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:184\u001b[0m, in \u001b[0;36m_recon_lcs.<locals>._recon\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m x[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m y[j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m [(x[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], i)]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m table[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j] \u001b[38;5;241m>\u001b[39m table[i, j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j)\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:188\u001b[0m, in \u001b[0;36m_recon_lcs.<locals>._recon\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:188\u001b[0m, in \u001b[0;36m_recon_lcs.<locals>._recon\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "    \u001b[1;31m[... skipping similar frames: _recon_lcs.<locals>._recon at line 188 (732 times)]\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:188\u001b[0m, in \u001b[0;36m_recon_lcs.<locals>._recon\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:184\u001b[0m, in \u001b[0;36m_recon_lcs.<locals>._recon\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m x[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m y[j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m [(x[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], i)]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m table[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j] \u001b[38;5;241m>\u001b[39m table[i, j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j)\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:188\u001b[0m, in \u001b[0;36m_recon_lcs.<locals>._recon\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:188\u001b[0m, in \u001b[0;36m_recon_lcs.<locals>._recon\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "    \u001b[1;31m[... skipping similar frames: _recon_lcs.<locals>._recon at line 188 (1768 times)]\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\rouge\\rouge_score.py:188\u001b[0m, in \u001b[0;36m_recon_lcs.<locals>._recon\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recon(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
          ]
        }
      ],
      "source": [
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_sentence = ' '.join([word.lower() for word in words if word.isalnum() and word.lower() not in stop_words])\n",
        "        filtered_sentences.append(filtered_sentence)\n",
        "    return filtered_sentences\n",
        "\n",
        "\n",
        "#def summarize_text(preprocessed_sentences, top_n=10):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
        "    scores = tfidf_matrix.sum(axis=0)\n",
        "    scored_sentences = [(score, sentence) for score, sentence in zip(scores.tolist()[0], preprocessed_sentences)]\n",
        "    scored_sentences.sort(reverse=True)\n",
        "    summarized = ' '.join([sentence for _, sentence in scored_sentences[:top_n]])\n",
        "    return summarized\n",
        "def summarize_text(preprocessed_sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
        "    \n",
        "    # Calculate sentence importance based on the sum of TF-IDF scores\n",
        "    scores = tfidf_matrix.sum(axis=1).flatten().tolist()[0]  # Ensure this is a flat list of scores\n",
        "    \n",
        "    # Normalize scores to fall between 0 and 1\n",
        "    max_score = max(scores) if max(scores) > 0 else 1\n",
        "    normalized_scores = [score / max_score for score in scores]\n",
        "    \n",
        "    # Determine cut-off score dynamically: mean score + 0.5 * std deviation of scores\n",
        "    mean_score = sum(normalized_scores) / len(normalized_scores)\n",
        "    std_dev_score = (sum([(x - mean_score) ** 2 for x in normalized_scores]) / len(normalized_scores)) ** 0.5\n",
        "    cut_off_score = mean_score + 0.2 * std_dev_score  # Tighter threshold\n",
        "\n",
        "    # Collect sentences that exceed the cut-off score\n",
        "    summarized_sentences = [sentence for score, sentence in zip(normalized_scores, preprocessed_sentences) if score >= cut_off_score]\n",
        "    \n",
        "    summarized = ' '.join(summarized_sentences)\n",
        "    \n",
        "    return summarized\n",
        "def evaluate_summary(original_text, summary):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(summary, original_text)\n",
        "    original_count = len(word_tokenize(original_text))\n",
        "    summary_count = len(word_tokenize(summary))\n",
        "    return scores, original_count, summary_count\n",
        "\n",
        "pdf_text = extract_text_from_pdf('../Prroject -Text Summarization/PC.pdf')\n",
        "preprocessed_sentences = preprocess_text(pdf_text)\n",
        "summary = summarize_text(preprocessed_sentences)\n",
        "scores, original_count, summary_count = evaluate_summary(pdf_text, summary)\n",
        "\n",
        "print(\"ROUGE Scores:\", scores)\n",
        "print(\"Original Word Count:\", original_count)\n",
        "print(\"Summary Word Count:\", summary_count)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOQq_ZpI5Vmv"
      },
      "source": [
        "# Advanced Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbMUVAeo5U6W",
        "outputId": "2fea2230-d970-4596-f28b-7068071b2f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted text length: 117508\n",
            "Summary: introduction parallel computing computer software written conventionally serial computing meant solve problem algorithm divides problem smaller instructions discrete instructions executed central processing unit computer one one one instruction finished next one starts reallife example would people standing queue waiting movie ticket cashier cashier giving tickets one one persons complexity situation increases 2 queues one cashier short serial computing following 1 problem statement broken discrete instructions 2 instructions executed one one 3 one instruction executed moment time look point 3 causing huge problem computing industry one instruction getting executed moment time huge waste hardware resources one part hardware running particular instruction time problem statements getting heavier bulkier amount time execution statements examples processors pentium 3 pentium 4 let  come back reallife problem could definitely say complexity decrease 2 queues 2 cashiers giving tickets 2 persons simultaneously example parallel computing parallel computing use multiple processing elements simultaneously solving problem problems broken instructions solved concurrently resource applied work working time advantages parallel computing serial computing follows 1 saves time money many resources working together reduce time cut potential costs 2 impractical solve larger problems serial computing 3 take advantage nonlocal resources local resources finite 4 serial computing  wastes  potential computing power thus parallel computing makes better work hardware types parallelism 1 bitlevel parallelism  form parallel computing based increasing processor  size reduces number instructions system must execute order perform task largesized data example consider scenario 8bit processor must compute sum two 16bit integers must first sum 8 lowerorder bits add 8 higher order bits thus requiring two instructions perform operation 16bit processor perform operation one instruction 2 instructionlevel parallelism  processor address less one instruction clock cycle phase instructions reordered grouped later executed concurrently without affecting result program called instruction level parallelism 3 task parallelism  task parallelism employs decomposition task subtasks allocating subtasks execution processors perform execution subtasks concurrently 4 datalevel parallelism dlp  instructions single stream operate concurrently several data  limited nonregular data manipulation patterns memory bandwidth parallel computing  whole realworld runs dynamic nature ie many things happen certain time different places concurrently data extensively huge manage  realworld data needs dynamic simulation modeling achieving parallel computing key  parallel computing provides concurrency saves time money  complex large datasets management organized using parallel computing  approach  ensures effective utilization resources hardware guaranteed used effectively whereas serial computation part hardware used rest rendered idle  also impractical implement realtime systems using serial computing applications parallel computing  databases data mining  realtime simulation systems  science engineering  advanced graphics augmented reality virtual reality limitations parallel computing  addresses communication synchronization multiple subtasks processes difficult achieve  algorithms must managed way handled parallel mechanism  algorithms programs must low coupling high cohesion  difficult create programs  technically skilled expert programmers code parallelismbased program well history parallel computing parallel computing evolved significantly decades  1950s60s early vector processing  1970s80s emergence multiprocessors simd mimd architectures  1990s introduction parallel clusters distributed computing  2000spresent multicore cpus gpus cloud computing frameworks hadoop spark etc 4 speedup speedup quantifies performance improvement parallel execution compared sequential execution mathematically factors affecting speedup  communication overhead time required processor communication  load imbalance unequal distribution tasks across processors  synchronization waiting processes reach specific states ideal speedup linear speedup indicates doubling processors halves runtime however practical speedup often sublinear due overheads 5 modeling parallel computation parallel computational models assist analyzing performance complexity notable models include pram parallel random access machine ideal theoretical model processors share common memory variants exclusive read exclusive write erew concurrent read exclusive write crew concurrent read concurrent write crcw dataparallel model  definition operations performed simultaneously large data sets partitioned among multiple processors executing identical instructions simultaneously simdsingle instruction multiple data  key features emphasizes parallelism data level suitable vector array operations  use graphics processing numerical simulations machine learning task graph model description  task graph directed acyclic graph dag used represent parallel computations  nodes represent tasks units computation  edges represent dependencies constraints specifying execution order key features  clearly specifies task dependencies concurrency  useful scheduling tasks analyzing parallelism example scenario  consider scenario compile software modules nodes compilation individual modules edges dependencies modules eg module b depends module advantages  explicit visualization dependencies parallelism  efficiently schedules tasks disadvantages  complex task graphs may incur overhead managing execution work pool task pool model description  independent tasks placed shared pool queue  workers dynamically fetch tasks execute usually managed threads key features  dynamic load balancing  highly effective tasks differ significantly execution time example scenario  web server handling client requests requests placed shared queue worker threadsprocesses dynamically handle request advantages  automatic load balancing  easy implement scale disadvantages  overhead synchronization contention accessing shared pool masterslave masterworker model description  one node acts master distributing tasks several slave worker nodes  workers process tasks independently report results back master key features  centralized control master node  effective heterogeneous systems example scenario  distributed image rendering master divides frames distributes workers workers render frames send back master advantages  simple manage implement  easily adaptable varying workloads disadvantages  potential bottleneck master node  single point failure pipeline model description  tasks broken stages output one stage input next  multiple stages run simultaneously processing different data items concurrently key features  parallel execution multiple stages  suitable repetitive tasks example scenario  multimedia stream processing stage 1 capture video frames stage 2 compression stage 3 transmission network advantages  high throughput efficient resource utilization  natural fit continuous data streams disadvantages  performance limited slowest stage pipeline bottleneck  balancing workloads among stages complex 6 multiprocessor models multiprocessor architectures categorized  shared memory multiprocessors smp single shared memory accessible multiple processors examples symmetric multiprocessors multicore cpus  distributed memory multiprocessors processor dedicated memory communication via explicit messagepassing eg mpi  hybrid architectures combination shared distributed memory architectures example clusters multicore processors 610 interconnection networks properties classification topologies combined explanation interconnection networks connect processors nodes enabling efficient communication  basic properties topology networks physicallogical arrangement latency delay sendingreceiving data bandwidth amount data transmitted per unit time scalability ability add nodes efficiently fault tolerance network robustness failures  classification static networks fixed connections bus ring mesh hypercube dynamic networks switchable networks crossbar multistage  topologies examples bus single shared line simple limited ring circular connections moderate scalability mesh gridlike connection highly scalable tree hierarchical connections hypercube node connected others multidimensional space high scalability low latency crossbar fully switchable matrix low latency expensive 11 parallel computational complexity parallel complexity assesses efficiency based  work w total operations performed  depth critical path length longest dependency chain analyzing helps optimize parallel algorithms balancing computational load effectively 12 brent  theorem brent  theorem provides runtime bound parallel algorithms tpwpdtp leq fracwp dtppwd  suggests parallel execution time depends work distribution wp plus critical path  indicates diminishing returns beyond optimal processor count detailed questions 15 marks q1 discuss detail evolution parallel computing inception modern era ans parallel computing evolved significantly since inception driven need solve increasingly complex problems faster efficiently early 1950s 1960s parallel computing concepts began rudimentary pipelining early vector processing systems like ibms stretch illiac iv introduced parallel concepts albeit limited hardware constraints time 1970s 1980s development simd single instruction multiple data mimd multiple instruction multiple data architectures became prevalent exemplified vector processors built cray research machines capable processing multiple data elements simultaneously significantly improving computational performance scientific military applications 1990s marked transition distributed parallel computing clusters becoming popular due affordability flexibility technologies like message passing interface mpi emerged allowing efficient internode communication beowulf clusters built commodity hardware became costeffective solution parallel computing needs democratizing access highperformance computing entering 2000s multicore processors became standard due plateau singlecore performance improvements cpus multiple cores provided inherent parallelism capabilities accessible broader user base additionally gpus became prominent generalpurpose computing gpgpu due massive parallel processing capabilities supported frameworks like cuda opencl modern era extended parallel computing cloud computing environments leveraging distributed infrastructure provide scalable flexible computational resources ondemand era also seen rise big data frameworks apache hadoop apache spark designed explicitly parallel processing vast datasets today parallel computing remains fundamental advancements ai scientific research realtime analytics computationally intensive fields q2 explain classification properties importance interconnection networks parallel computing suitable diagrams ans interconnection networks play critical role parallel computing facilitating communication among processors memory modules storage units networks broadly classified two main categories static dynamic static networks fixed interconnections offering predictable structured communication paths examples include mesh ring tree hypercube topologies instance mesh topology connects processors gridlike arrangement enabling efficient neighbour communication hypercube connects processors multidimensional cube formations node directly connects nodes differing one bit binary addresses providing excellent scalability low latency dynamic networks hand employ switchable configurable connections adapting dynamically varying communication demands examples include crossbar multistage interconnection networks crossbar networks provide direct connections inputs outputs via switches offering high bandwidth low latency higher cost complexity key properties interconnection networks include  latency time required transmit message source destination  bandwidth maximum data transfer rate supported network  scalability ability effectively integrate additional processors  fault tolerance capability maintain operations despite node link failures interconnection networks crucial parallel systems directly influence performance efficiency scalability system effective networks ensure minimal latency high bandwidth critical applications requiring frequent largescale data transfers among processors q3 describe amdahls law brents theorem detail emphasizing significance implications practical limitations parallel systems ans formula gives theoretical speedup latency execution task fixed workload expected system whose resources improved words formula used find maximum improvement possible improving particular part system often used parallel computing predict theoretical speedup using multiple processors speedup speedup defined ratio performance entire task using enhancement performance entire task without using enhancement speedup defined ratio execution time entire task without using enhancement execution time entire task using enhancement pe performance entire task using enhancement possible pw performance entire task without using enhancement ew execution time entire task without using enhancement ee execution time entire task using enhancement possible speedup pepw speedup ewee amdahl  law uses two factors find speedup enhancement  fraction enhanced  fraction computation time original computer converted take advantage enhancement example 10 seconds execution time program takes 40 seconds total use enhancement fraction 1040 obtained value fraction enhanced fraction enhanced always less 1  speedup enhanced  improvement gained enhanced execution mode much faster task would run enhanced mode used entire program example  enhanced mode takes say 3 seconds portion program 6 seconds original mode improvement 63 value speedup enhanced speedup enhanced always greater 1 formula amdahl  law 1 1  p p n speedup system p proportion system improved n number processors system advantages amdahl  law  provides way quantify maximum potential speedup achieved parallelizing program help guide decisions hardware software design  helps identify portions program easily parallelizable guide efforts optimize portions code  provides framework understanding tradeoffs parallelization forms optimization code optimization algorithmic improvements disadvantages amdahl  law  assumes portion program parallelized fixed may case practice example possible optimize code reduce portion program parallelized making amdahl  law less accurate  assumes processors performance characteristics may case practice example heterogeneous computing environment processors may faster others affect potential speedup achieved  take account factors affect performance parallel programs communication overhead load balancing factors impact actual speedup achieved practice may lower theoretical maximum predicted amdahl  law brents theorem provides practical bound parallel runtime number processors limited theorem states total work operations depth critical path length number processors brent  theorem implies adding processors reduces execution time reaching critical path constraint practically means optimal number processors beyond performance improvement stalls due overheads dependencies amdahl  law brent  theorem provide crucial insights designing efficient parallel algorithms hardware indicating improvements parallel systems must carefully balance sequential constraints overhead management resource utilization q4 explain different multiprocessor models diagrams discuss advantages disadvantages ans multiprocessor models categorized mainly shared memory distributed memory hybrid models shared memory model model processors access common memory space simplifies programming data sharing introduce memory contention scalability challenges examples include symmetric multiprocessing smp systems typically multicore cpus distributed memory model processor private memory communication handled explicitly messagepassing mechanisms like mpi offers excellent scalability avoids memory contention complicates programming due explicit message handling hybrid model model integrates shared distributed memory approaches clusters composed multicore nodes represent hybrid models leveraging easy intranode programming shared memory scalable internode communication distributed memory advantages disadvantages  shared memory easy programming efficient data sharing limited scalability  distributed memory high scalability less contention complex programming  hybrid balances ease programming scalability yet complex manage q5 discuss different types parallelism suitable examples scenarios type ans several key types parallelism 1 instructionlevel parallelism ilp multiple instructions executed simultaneously within processors using pipelines superscalar architectures example modern cpus executing multiple arithmetic operations simultaneously 2 datalevel parallelism dlp identical operations performed concurrently multiple data elements example gpus performing image rendering applying computation pixels simultaneously 3 tasklevel parallelism tlp independent tasks processes executed concurrently different processors example running multiple applications simultaneously multicore processor browsing internet editing documents virus scanning 4 bitlevel parallelism blp concurrent processing multiple bits within single instructions example cpus processing 32bit 64bit instructions handling multiple bits parallel enhance throughput type parallelism addresses different computational scenarios collectively enhancing overall computational efficiency performance unit 2 challenges programming multicore system multicore system consists two processors attached single chip enhance performance reduce power consumption efficient simultaneous processing multiple tasks multicore system recent trend core appears separate processor multicore system capable executing one threads parallelly whereas single core system one thread execute time implementing multicore system beneficial implementing single core system increasing number transistors single chip enhance performance increasing number transistors single chip increases complexity system challenges multicore system since multicore system consists one processor need keep busy make better use multiple computing cores scheduling algorithms must designed use multiple computing core allow parallel computation challenge also modify existing new programs multithreaded take advantage multicore system general five areas present challenges programming multicore systems 1 dividing activities challenge examine task properly find areas divided separate concurrent subtasks execute parallelly individual processors make complete use multiple computing cores 2 balance dividing task subtasks equality must ensured every subtask perform almost equal amount work case one sub task lot work perform sub tasks less case multicore system programming may enhance performance compared single core system 3 data splitting task divided smaller subtasks data accessed manipulated task must also divided run different cores data easily accessible subtasks 4 data dependency since various smaller subtasks run different cores may possible one subtask depends data another sub tasks data needs examined properly execution whole task synchronized 5 testing debugging different smaller subtasks executing parallelly testing debugging concurrent tasks difficult testing debugging single threaded application shared memory programming model shared memory programming model parallel computing approach multiple processors cores operate independently share single coherent memory space accessible processors model allows processors communicate efficiently reading writing common memory locations significantly simplifying data exchange compared distributed memory models require explicit message passing distributed model b shared model shared memory architectures typically employ symmetric multiprocessing smp nonuniform memory access numa configurations smp systems processor equal access uniform memory ensuring consistent predictable memory access latency numa systems however organize memory nodes providing faster access local memory slower access remote memory thus affecting application performance managed carefully key advantage shared memory model lies ease programming developers need explicitly manage communication instead synchronization managed using various mechanisms like mutexes semaphores barriers locks control access shared resources avoiding conflicts ensuring data integrity despite advantages shared memory model inherent limitations including scalability issues due memory contention synchronization overhead number processors increases contention memory resources also increases potentially leading performance degradation synchronization data sharing carefully optimized consequently developers must employ techniques data partitioning load balancing minimizing synchronization achieve optimal performance common programming interfaces shared memory models include openmp posix threads pthreads java threads providing varying levels abstraction control openmp instance simplifies parallel programming compiler directives reducing development complexity maintaining significant performance improvements shared memory programming model remains popular multicore processors servers workstations due straightforward implementation performance benefits widespread hardware support 3 multithreaded programs using openmp multithreaded programming involves executing multiple threads concurrently within single process allowing efficient utilization multicore processors openmp significantly simplifies writing multithreaded programs abstracting thread management synchronization compiler directives runtime routines environment variables openmp threads share processs memory space facilitating efficient direct data sharing among threads enhancing performance reducing communication overhead create multithreaded programs openmp programmers use directives pragma omp parallel define parallel regions within regions openmp automatically generates multiple threads execute concurrently programmers control parallelism granularity specifying clauses numthreads allowing explicit control number threads relying openmp defaults optimized available hardware synchronization openmp multithreaded programs essential maintain data consistency prevent race conditions openmp provides various synchronization mechanisms including barriers critical sections locks atomic operations barriers ensure threads synchronize specific points waiting proceeding critical sections allow exclusive access shared resources preventing simultaneous modifications multiple threads atomic operations provide lightweight synchronization ensuring threadsafe updates shared variables without overhead critical sections performance tuning openmp multithreaded programs involves managing load balancing minimizing overhead effective load balancing ensures equal distribution computational tasks among threads preventing scenarios threads remain idle others overloaded programmers achieve load balancing using scheduling clauses static dynamic guided depending workload characteristics additionally minimizing synchronization overhead careful program structuring data partitioning significantly enhances performance openmp  ease use portability effectiveness managing threads synchronization make preferred choice parallelizing computationally intensive applications scientific simulations realtime processing tasks loop level parallelism computer architecture since beginning multiprocessors programmers faced challenge take advantage power process available sometimes parallelism available present form complicated programmer think addition exists large sequential code years incremental performance improvements afforded advancement singlecore execution long time automatic parallelization seen good solution challenges parallelization removes programmer  burden expressing understanding parallelism existing algorithm looplevel parallelism computer architecture helps us taking parallel tasks within loops order speed process utility parallelism arises data stored random access data structures like arrays program runs sequence iterate array perform operations indices time program looplevel parallelism use multi threads multiprocesses operate indices time different times loop level parallelism types 1 doall parallelismindependent multithreading imt 2 doacross parallelismcyclic multithreading cmt 3 dopipe parallelismpipelined multithreading pmt 1 doall parallelismindependent multithreading imt doall parallelism every iteration loop executed parallel completely independently interthread communication iterations assigned threads round robin fashion example 4 cores core 0 execute iterations 0 4 8 12 etc see figure type parallelization possible loop contain loopcarried dependencies changed conflicts occur simultaneous iterations executing loops parallelized way likely experience speedups since overhead interthread communication however lack communication also limits applicability technique many loops amenable form parallelization 2 doacross parallelismcyclic multithreading cmt doacross parallelism like independent multithreading assigns iterations threads roundrobin manner optimization techniques described increase parallelism independent multi threading loops also available cyclic multithreading technique dependencies identified compiler beginning loop iteration delayed till dependencies previous iterations satisfied manner parallel portion one iteration overlapped sequential portion subsequent iteration result ends parallel execution example figure statement x xnext causes loop carried dependence since evaluated statement completed previous iteration cores started first iteration approach linear speedup parallel part loop large allow full utilization cores 3 dopipe parallelismpipeline multithreading pmt dopipe parallelism way parallelization loops cross iteration dependencies approach loop body divided number pipeline stages pipeline stage assigned different core iteration loop distributed across cores stage loop executed core assigned pipeline stage individual core executes code associated stage allocated instance figure loop body divided 4 stages b c iteration distributed across four cores stage executed one core openmp loop parallelization achieved using directives like pragma omp directive instructs compiler divide loop iterations among multiple threads automatically ensure correct parallel execution loop iterations must independent execution one iteration must affect depend results another iteration independent iterations executed simultaneously without synchronization enabling maximum parallel efficiency distributing iterations among threads crucial step loop parallelization openmp provides several scheduling options manage distribution effectively default scheduling openmp static loop iterations evenly divided among threads execution begins static scheduling efficient loops iteration similar computational loads ensuring balanced workload distribution minimizing overhead dynamic scheduling another scheduling option provided openmp distributes loop iterations threads runtime complete previous iterations approach particularly effective loops variable iteration workloads enhancing load balancing introducing runtime overhead guided scheduling hybrid approach initially distributing larger chunks iterations threads progressively reducing chunk size execution proceeds method combines benefits static dynamic scheduling providing initial efficiency better load balancing towards loops end proper parallelization loops also involves handling loopcarried dependencies effectively openmp provides mechanisms reduction operations accumulating results safely across threads barriers synchronization specific points privateshared clauses manage variable scoping ensuring loop iterations independence proper thread distribution critical efficient parallel execution maximizing performance gains parallel computing environments 5 reductions reductions common operations parallel computing used combine values computed parallel threads single resultant value operations like summation multiplication finding minimum maximum logical operations like andor typically require reductions openmp provides straightforward efficient mechanism handling reduction operations within parallel regions ensuring threadsafe computation without explicit synchronization management openmp reduction operation specified using reduction clause within directives like pragma omp reduction syntax straightforward allowing programmer indicate operation variable reduce example summation reduction would specified reductionsum execution openmp generates private copies reduction variable thread thread independently computes partial results using private variable completing parallel computation openmp automatically combines private copies single shared variable using specified reduction operation reduction operations significantly improve parallel execution efficiency avoiding explicit locks synchronization constructs like critical sections mutexes explicit synchronization introduce overhead contention reduced scalability openmps builtin reductions minimize overhead utilizing optimized implementations provided compilers leading superior performance common examples reduction operations practical applications include summing elements large arrays calculating global minima maxima performing logical evaluations aggregating statistics data processing tasks utilizing reductions parallel algorithms ensures accurate results efficient computation simplicity efficiency openmps reduction mechanisms made reductions widely adopted computationally intensive domains like scientific computing financial modeling data analytics simulations performance accuracy paramount 6 parallel tasks parallel tasks openmp refer independent units work executed concurrently threads within parallel region unlike parallel loops distribute iterations across threads systematically parallel tasks allow developers define explicitly independent operations executed concurrently approach provides greater flexibility control enabling dynamic adaptive parallel execution strategies suited complex irregular workloads tasks created using pragma omp task directive within parallel region defined pragma omp parallel thread encounters task directive packages associated code block task places shared task queue threads within parallel region dynamically pick tasks queue execute facilitating effective load balancing particularly beneficial tasks varying computational demands openmp also provides task synchronization mechanisms pragma omp taskwait directive ensures child tasks generated thread complete proceeding task synchronization crucial maintaining correctness task dependent workflows parallel tasks particularly effective applications recursive structures irregular computations scenarios requiring dynamic parallelism examples include traversing complex data structures like trees graphs performing recursive divideandconquer algorithms dynamically adapting computation based runtime conditions using parallel tasks developers achieve improved parallel efficiency better load balancing increased utilization computing resources flexible dynamic nature task parallelism enables efficient handling workloads inherently unstructured exhibit unpredictable runtime behavior making tasks essential component advanced parallel programming openmp environments q1 briefly explain openmp simplifies programming multicore shared memory multiprocessors openmp open multiprocessing application programming interface api designed specifically simplify parallel programming multicore shared memory multiprocessor systems provides standardized portable efficient framework programmers develop parallel applications easily traditional parallel programming requires explicit management threads synchronization workload distribution memory consistency often introduces complexity errors inefficiencies openmp significantly reduces complexity higherlevel abstraction model one primary ways openmp simplifies programming compiler directives compiler directives pragma omp parallel allow programmers indicate parallel regions explicitly leaving underlying complexity thread creation management synchronization compiler simply annotating sequential code directives programmers easily transform parallelized code without rewriting logic scratch openmp utilizes shared memory programming model allows threads access common memory space directly model naturally simplifies data sharing eliminating explicit messagepassing requirements common distributed memory systems variables marked shared private using simple clauses making memory management straightforward intuitive programmers explicitly allocate manage distributed memories handle complex communication patterns processors moreover openmp automatically manages workload distribution among available cores using scheduling clauses programmers easily specify static dynamic guided scheduling loops allowing efficient balanced execution without detailed manual intervention simplifies handling workload imbalances significantly common challenge parallel programming synchronization another critical feature simplified openmp provides builtin synchronization constructs barriers critical sections atomic operations locks abstracting complex synchronization logic helping prevent race conditions concurrency issues significantly reduces programmer effort ensuring correct parallel execution another important simplification arises openmp  incremental parallelism approach programmers incrementally parallelize code initially identifying performancecritical sections gradually optimizing rest progressive approach allows performance improvements without immediate comprehensive restructuring additionally openmp offers portability across various platforms compilers allowing code written run effectively across multiple hardware architectures portability significantly simplifies development maintenance parallel applications across different systems overall openmp provides abstraction automation standardized syntax portability greatly reducing parallel programming complexity managing thread creation memory sharing workload distribution synchronization incremental parallelization intuitive directives openmp enables programmers effectively leverage multicore shared memory multiprocessor systems without extensive parallel programming expertise q2 define shared memory programming model discuss one advantage one limitation shared memory programming model refers parallel computing architecture multiple processors cores share single global memory space model threads directly read write common memory processors equal access shared memory communication among processors threads occurs implicitly memory references making data sharing simpler intuitive compared distributed memory models rely explicit message passing shared memory programming model parallel applications typically use threads rather separate processes simplifies data management reduces overhead enhances ease parallelization popular implementations model include openmp pthreads offering easy ways write parallel programs multicore systems advantage one significant advantage shared memory model simplicity communication data sharing memory shared among processors threads programmer need explicitly handle message passing data distribution across different nodes processors instead threads naturally share data structures leading simpler programming easier code management simplicity translates faster development cycles reduced programmer effort moreover debugging shared memory applications tends easier compared distributed memory models data structures memory states directly observable single memory space limitation key limitation shared memory programming model scalability number processors increases contention shared memory significantly increases creating bottlenecks performance degradation memory bandwidth latency become serious issues multiple threads attempt simultaneously access modify memory locations causing performance limitations due cache coherence overhead synchronization constraints additionally shared memory architectures typically physical limits regarding number processors cores effectively supported within single memory space consequently highly efficient systems modest number processors cores shared memory programming models become increasingly less effective less efficient system scale grows therefore shared memory programming model provides easy programmability intuitive data sharing faces significant challenges achieving scalability large processor systems thereby limiting applicability relatively moderatescale parallel computing systems q3 describe multithreading implemented openmp multithreading openmp implemented primarily compiler directives runtime library routines environment variables abstract away complexity involved explicitly managing threads unlike traditional thread libraries like posix threads pthreads require explicit creation management threads openmp handles tasks automatically enabling programmers focus parallelizing algorithms rather thread management core openmp multithreading concept parallel regions defined pragma omp parallel directive program execution reaches parallel region openmp automatically creates multiple threads executing copy enclosed code block concurrently upon completion parallel region threads implicitly synchronized managed openmp significantly simplifying parallel programming openmp thread unique thread id retrievable using runtime function ompgetthreadnum id utilized within parallel code regions differentiate threads manage workload distribution explicitly required total number threads parallel region also controlled either programmatically runtime library routines ompsetnumthreads via environment variables ompnumthreads allowing flexible adaptive resource utilization furthermore openmp supports advanced multithreading clauses private shared variables reduction operations barriers scheduling example private clause specifies threadprivate variables ensuring thread maintains separate copy thereby preventing unintended data sharing race conditions conversely variables declared shared remain accessible threads facilitating efficient simple data sharing parallel regions looplevel parallelism another essential aspect multithreading implementation openmp achieved using directive pragma omp parallel directive automatically divides loop iterations among multiple threads ensuring efficient execution independent iterations parallel thereby improving performance significantly openmp also provides explicit synchronization mechanisms critical sections atomic operations barriers ensuring correct deterministic behavior multithreaded applications mechanisms allow controlled access shared resources preventing race conditions data inconsistencies overall multithreading implementation openmp significantly simplifies parallel programming abstracting thread creation workload distribution data sharing synchronization userfriendly directives runtime environment facilitate efficient utilization multicore multiprocessor architectures without detailed knowledge low level threading mechanisms making openmp highly suitable developing high performance parallel applications q4 independent iterations loop parallelization crucial efficient parallel execution parallel computing particularly using openmp independent iterations refer iterations loop dependency meaning execution outcome one iteration affect require result another independence iterations implies loopcarried dependencies makes possible iterations run concurrently across multiple threads without causing race conditions synchronization issues incorrect computations identification independent iterations fundamental step parallelizing loops directly impacts efficiency correctness effectiveness parallel programs loops contain iterations inherently independent parallelizing yield significant performance improvements due maximum parallel execution potential one crucial reason independent iterations essential efficient parallel execution elimination reduction synchronization overhead loops dependent iterations synchronization mechanisms barriers critical sections locks required ensure correct execution order causing overhead reducing efficiency however independent iterations naturally avoid synchronization issues allowing threads execute without waiting locking thus maximizing cpu utilization furthermore independent iterations facilitate better workload distribution load balancing since iteration executed independently workload evenly distributed across available threads processors ensuring balanced resource utilization minimizing idle time balanced distribution particularly beneficial iteration workloads uniform predictable allows simple scheduling techniques static scheduling achieve nearoptimal parallel efficiency independent iterations also contribute significantly scalability number processors cores increases loops containing independent iterations effectively exploit additional computing resources without incurring significant communication synchronization penalties consequently program scale almost linearly leveraging increased processing power available modern multiprocessor multicore systems additionally parallelizing independent iterations simplifies parallelization process using openmp programmers easily identify independent loops use straightforward parallelization directives pragma omp parallel compiler automatically manages thread creation iteration assignment workload distribution automatic management simplifies parallel programming reduces potential errors significantly shortens development time conversely loops dependent iterations require complicated analysis transformations loop transformations eg loop interchange loop fusion loop splitting advanced synchronization techniques achieve parallelization complexity often diminishes efficiency benefits parallel execution increases likelihood parallelization errors summary independent iterations fundamental efficient parallel execution minimize synchronization overhead enable optimal workload distribution enhance scalability significantly simplify parallelization process collectively contribute maximum parallel performance effective utilization computing resources q5 explain difference static dynamic scheduling loop iterations openmp openmp scheduling determines loop iterations distributed among available threads parallel execution two primary scheduling approaches openmp static scheduling dynamic scheduling distinct characteristics usecases advantages limitations static scheduling static scheduling loop iterations divided evenly among threads execution begins division remains fixed throughout execution loop thread knows precisely iterations handle advance minimizing runtime overhead due assignment synchronization static scheduling especially suitable loop iterations relatively uniform execution times meaning iteration takes approximately equal computational effort conditions static scheduling achieves excellent load balancing minimal overhead maximizing efficiency parallel performance example loop 100 iterations four threads available static scheduling assigns iterations 024 thread 0 2549 thread 1 5074 thread 2 7599 thread 3 primary advantage static scheduling minimal runtime overhead iterations predetermined however main limitation emerges cases varying workloads per iteration loop iterations significantly differ execution time static scheduling may lead load imbalance threads finish early remain idle resulting suboptimal resource utilization dynamic scheduling dynamic scheduling assigns loop iterations threads runtime dynamically threads receive iterations individually small chunks finish processing previously assigned iterations dynamic assignment continues loop iterations processed dynamic scheduling effectively handles loops uneven unpredictable iteration workloads ensuring better load balancing compared static scheduling since threads request new iterations available naturally adapts runtime variations evenly distributing computational load across threads however flexibility introduces additional runtime overhead due frequent scheduling synchronization instance dynamic scheduling four threads processing 100 iterations threads initially receive smaller chunks iterations thread completes assigned iterations dynamically requests remaining iterations thereby ensuring consistent workload distribution minimizing idle times even significant workload variance across iterations comparison usecases static scheduling typically preferred loop iterations predictable uniform offers minimal overhead efficient execution examples include numerical computations array operations uniform workloads dynamic scheduling preferable iteration execution times vary significantly complex simulations irregular data processing loops involving conditionally expensive computations despite introducing scheduling overhead dynamic scheduling often yields superior performance scenarios avoiding load imbalance conclusion static scheduling optimal uniform workloads predictable computational costs offering low overhead limited flexibility conversely dynamic scheduling effectively manages irregular workloads providing better load balancing incurring higher runtime overhead due dynamic management q6 describe reduction operations openmp example openmp reduction operation aggregates values computed independently multiple threads single shared result safely efficiently reduction operations particularly useful parallel loops threads individually compute partial results must combined summation multiplication finding minimum maximum values logical operations reduction operations openmp specified using reduction clause ensures thread maintains private copy reduction variable preventing race conditions enabling efficient parallel computation parallel execution openmp automatically combines private copies final shared result using specified operation example consider calculating sum elements large array parallel without reduction multiple threads updating shared sum simultaneously would cause race conditions reduction safely addresses issue double sum 00 pragma omp parallel reductionsum forint 0 n sum arrayi example thread independently computes partial sum assigned loop iterations storing result threadprivate copy variable sum threads complete execution openmp automatically combines private sums final global sum using addition approach ensures correct efficient parallel computation without explicit synchronization openmp supports various reduction operations including arithmetic logical bitwise operations programmers explicitly specify reduction operation reduction clause allowing flexible intuitive parallel reductions reduction operations significantly simplify parallel programming automating data aggregation eliminating manual synchronization ensuring correctness without reduction programmers must explicitly handle private variables synchronization manual accumulation increasing complexity risk errors thus reductions offer safe efficient straightforward way aggregate data computed parallel q7 explain purpose parallel tasks openmp provide example scenario beneficial openmp task represents specific unit work computation executed independently threads parallel openmp tasks used primarily parallelize applications workload naturally fit standard loop parallelism unlike looplevel parallelism works best uniformly distributed independent iterations task parallelism effectively handle irregular dynamic unpredictable workloads tasks openmp explicitly defined programmer using pragma omp task directive thread encounters directive associated block code encapsulated task openmp runtime system schedules dynamically execution available threads approach provides flexibility workload distribution allowing tasks execute asynchronously independently thus maximizing concurrency resource utilization one primary benefit using tasks openmp ability handle recursive dynamically generated workloads efficiently tasks dynamically spawn tasks making wellsuited recursive algorithms quicksort mergesort recursive tree traversal algorithms typically generate workloads dynamically irregularly traditional loopbased parallelization methods inadequate instance consider scenario involving recursive parallelization traversing processing nodes binary tree nodes traversal may treated separate parallel task dynamically generating new tasks child nodes using openmp tasks expressed clearly efficiently void traversenode n n null return pragma omp task traversenleft pragma omp task traversenright processn pragma omp taskwait pragma omp parallel pragma omp single traverseroot example node processed spawns new tasks dynamically runtime scheduler distributes tasks across available threads taskwait directive ensures synchronization critical points guaranteeing correctness efficient execution tasks also significantly enhance load balancing parallel programs irregular workloads since tasks dynamically scheduled runtime assigns threads based availability workload reducing idle times achieving better resource utilization compared static scheduling additionally tasks enable improved parallelization granularity control programmers finely tune granularity tasks creating smaller tasks greater parallelism larger tasks reduce overhead depending nature workload summary parallel tasks openmp provide robust mechanism handle irregular dynamic recursive parallel workloads ensuring efficient concurrency improved load balancing simplified parallelization complex applications q8 guided scheduling openmp scenario would preferred static scheduling guided scheduling openmp loop iteration scheduling strategy designed dynamically distribute loop iterations among threads combining advantages static dynamic scheduling guided scheduling assigns loop iterations threads progressively smaller chunks initially threads assigned relatively large blocks iterations execution progresses size chunks gradually decreases eventually reaching specified minimum chunk size guided scheduling specified openmp follows pragma omp parallel scheduleguided chunksize forint 0 n processi chunk size starts large often determined dividing remaining iterations number threads decreases gradually specified minimum chunk size chunksize advantage guided scheduling combines efficiency static scheduling large initial chunks minimize runtime overhead load balancing benefits dynamic scheduling later smaller chunks ensure threads remain balanced workloads vary towards end execution assigning larger chunks initially guided scheduling reduces runtime overhead associated frequent dynamic assignment subsequently smaller chunks become available threads finishing tasks early efficiently take new work thereby reducing idle time ensuring balanced utilization computational resources scenario guided scheduling preferred static scheduling occurs iteration workloads unpredictable vary significantly instance suppose loop performs operations may vary significantly computation time due conditional logic irregular data structures static scheduling would distribute iterations evenly among threads without considering runtime differences potentially resulting severe load imbalance threads would complete workload early remain idle others continue executing longer iterations guided scheduling addresses imbalance effectively initially distributing large iteration blocks reduces scheduling overhead subsequent smaller chunks enable threads pick additional work dynamically flexibility significantly improves load balancing overall performance irregular workloads example processing dynamically sized data blocks loops involving computations variablelength numerical simulations complex database queries benefit significantly guided scheduling adapts efficiently runtime variability ensuring balanced thread workloads reducing overall execution time summary guided scheduling optimal loop iterations exhibit significant runtime variability hybrid approach efficiently balances initial overhead load balancing making preferable static scheduling complex irregular parallel loops q9 synchronization important openmp mention two synchronization mechanisms provided openmp synchronization openmp essential ensure correctness consistency deterministic behavior parallel applications openmp programs execute simultaneously multiple threads accessing shared resources improper synchronization may result race conditions data corruption nondeterministic program behavior proper synchronization mechanisms allow threads coordinate safely access shared resources consistently maintain data integrity throughout parallel execution two essential synchronization mechanisms provided openmp critical sections barriers critical sections pragma omp critical critical section openmp ensures one thread execute particular block code given time mechanism crucial threads must safely update shared variables resources preventing simultaneous access modifications could lead race conditions example int sharedcounter 0 pragma omp parallel pragma omp critical sharedcounter example multiple threads attempt increment shared counter critical directive ensures one thread time accesses increment operation preventing incorrect outcomes data races barriers pragma omp barrier barrier synchronization enforces threads within parallel region must reach specific execution point thread continues acts synchronization checkpoint ensuring threads completed assigned tasks proceeding example pragma omp parallel performtask pragma omp barrier performnexttask threads execute performtask synchronize barrier thread moves beyond barrier performnexttask threads reached point barriers essential coordinate phases computation subsequent steps depend completion previous tasks across threads mechanismscritical sections barrierssignificantly impact correctness performance openmp programs critical sections ensure mutual exclusion avoid race conditions barriers provide explicit synchronization points essential maintaining program logic dependencies among parallel tasks however excessive improper use mechanisms introduce performance bottlenecks thread waiting overhead emphasizing importance careful synchronization design q10 discuss role task queues managing parallel tasks openmp task queues openmp internal runtime data structures responsible managing scheduling distributing dynamically created tasks among available threads role task queues fundamental efficient execution optimal resource utilization task based parallelism provided openmp thread encounters pragma omp task directive openmp runtime encapsulates code segment task object placing global threadlocal task queue threads within parallel region dynamically retrieve tasks queues executing asynchronously concurrently task queues thus serve critical intermediaries task creation execution ensuring effective task management scheduling one primary roles task queues enabling efficient load balancing tasks particularly dynamically generated runtime often vary significantly execution time dynamic nature task queues allows threads completing tasks earlier promptly retrieve additional tasks queues preventing thread idling ensuring balanced workload distribution furthermore task queues facilitate dynamic flexible parallelism unlike loopbased parallelism predetermined iteration distribution tasks dynamically generated managed queues task queues enable adaptive flexible workload distribution allowing parallel programs handle irregular recursive unpredictable computations efficiently openmp utilizes sophisticated scheduling algorithms manage task queues effectively including strategies task stealing idle threads take tasks threads queues ensure continuous resource utilization summary task queues fundamental openmps taskbased parallel model enable dynamic parallelism adaptive scheduling efficient load balancing simplified management irregular workloads ensuring effective parallel execution optimal resource utilization across parallel threads q1 discuss detail architecture programming strategies used multicore shared memory multiprocessors using openmp provide examples real world applications benefiting openmp architecture multicore shared memory multiprocessors multicore processors integrated circuits containing two independent processor cores core independently execute instructions significantly enhancing performance parallelizing computations shared memory multiprocessors refer systems multiple processor cores cpus share single unified memory space enabling direct data access without explicit data transfer sharedmemory architecture simplifies parallel programming allowing threads communicate implicitly shared variables components shared memory architecture  processor cores individual computing units capable independent instruction execution  shared memory common memory accessible processor cores facilitating rapid interthread communication data sharing  cache hierarchy multilevel caches l1 l2 l3 designed minimize memory latency optimize performance cache coherence mechanisms ensure consistency across cores  memory controller coordinates access shared memory handling synchronization coherence efficient memory allocation programming strategies using openmp openmp open multiprocessing standardized api tailored sharedmemory parallel programming designed abstract complexities thread management synchronization memory consistency key strategies include  parallel regions using directives pragma omp parallel define regions code executed simultaneously multiple threads  pragma omp parallel   printfthread id dn ompgetthreadnum   loop parallelization openmp simplifies parallelizing loops directive pragma omp parallel automatically distributing iterations among threads  synchronization mechanisms openmp provides synchronization constructs like barriers pragma omp barrier critical sections pragma omp critical atomic operations ensure correct parallel execution  workload scheduling openmp allows static dynamic guided scheduling effectively balance workload across threads based computation characteristics realworld applications benefiting openmp  scientific simulations weather forecasting climate modeling computational fluid dynamics cfd molecular dynamics simulations benefit loop parallelization reduction operations provided openmp  image signal processing tasks like image filtering video encodingdecoding audio processing leverage openmps looplevel parallelism significantly reducing processing time  data analytics machine learning openmp facilitates parallel processing large datasets accelerating algorithms matrix multiplication neural network training statistical computations  financial modeling complex financial models involving monte carlo simulations benefit parallel loops reductions provided openmp drastically reducing computational time  bioinformatics genome sequencing sequence alignment protein folding simulations rely heavily parallel computation achieve rapid results openmp efficiently addresses computational demands openmp  simplicity performance portability make widely adopted parallel programming model multicore architectures significantly benefiting various computationally intensive domains q2 explain shared memory programming model detail include characteristics advantages disadvantages typical use cases support answer suitable diagrams shared memory programming model shared memory programming model involves multiple processor cores accessing common memory space directly enabling threads communicate implicitly reading writing shared variables key characteristics  single memory address space processor cores share one physical memory accessible directly without explicit data transfers  implicit communication threads communicate implicitly shared variables greatly simplifying data sharing  threadbased parallelism programs typically utilize lightweight threads managed runtime operating system enabling fast context switching efficient resource utilization advantages  simplified programming model implicit data sharing simplifies parallel programming eliminating explicit message passing  low communication latency direct access shared memory enables rapid data transfer threads significantly reducing latency compared distributed models  ease debugging easier debugging due shared memory visibility allowing inspection memory states thread interactions straightforwardly disadvantages  limited scalability performance bottlenecks emerge number processors increases due memory contention cache coherence overhead  complex synchronization risk race conditions requires explicit synchronization mechanisms potentially reducing parallel efficiency  cache coherence overhead maintaining consistency data caches across multiple cores introduces significant performance overhead use cases  highperformance computing hpc scientific simulations numerical methods require intensive communication among threads  desktop workstation applications software requiring parallel processing single physical machines multimedia gaming realtime applications  serverside applications multithreaded web servers database engines transaction processing applications benefit rapid shared data access diagram shared memory multiprocessor model core 1 core 2 core 3 core 4 shared memory ram diagram shows multiple processor cores accessing shared memory directly illustrating ease immediacy data sharing q3 describe multithreaded programming openmp explain threads managed scheduled synchronized along benefits challenges involved multithreaded programming openmp multithreading openmp based explicit parallel regions multiple threads concurrently execute portions program openmp abstracts thread management highlevel compiler directives significantly simplifying parallel application development thread management openmp automatically creates manages terminates threads upon encountering parallel directives pragma omp parallel thread creation destruction occur dynamically runtime efficiently managed openmp runtime environment threads access unique thread ids via runtime library calls like ompgetthreadnum thread scheduling threads openmp scheduled using various strategies  static scheduling iterations evenly distributed execution starts minimal overhead  dynamic scheduling iterations assigned dynamically runtime balancing irregular workloads  guided scheduling initially assigns larger chunks gradually reducing chunk sizes achieving balance static efficiency dynamic flexibility synchronization openmp provides essential synchronization mechanisms  critical sections pragma omp critical ensures mutual exclusion preventing concurrent access shared resources  barriers pragma omp barrier synchronizes threads ensuring reach synchronization point proceeding  atomic operations protects simple shared variable updates minimal overhead benefits  ease parallelization highlevel directives abstract complex thread operations enabling rapid parallel application development  incremental parallelism allows gradual parallelization sequential code facilitating progressive optimization  portability openmp code easily runs across different hardware platforms without modification challenges  synchronization overhead excessive improper synchronization significantly reduces performance  load imbalance poor workload distribution among threads may cause inefficient resource utilization  race conditions without careful programming threads concurrently modifying shared variables may cause errors unpredictable behavior q4 discuss loop parallelization using openmp comprehensively explain independent iterations methods distributing iterations among threads discuss different scheduling strategies static dynamic guided examples loop parallelization openmp openmp simplifies loop parallelization automatically distributing iterations among multiple threads using pragma omp parallel independent iterations loops effectively parallelized iterations must independent iteration depends another independence eliminates synchronization overhead potential race conditions crucial efficient parallel execution distributing iterations among threads openmp automatically handles iteration distribution among threads based scheduling strategies  static scheduling fixed uniform distribution iterations ideal uniform workloads  dynamic scheduling iterations dynamically assigned runtime beneficial uneven workloads  guided scheduling combines static dynamic approaches assigning larger chunks initially smaller chunks later optimizing overhead load balance examples static scheduling pragma omp parallel schedulestatic forint i0 processi dynamic scheduling pragma omp parallel scheduledynamic forint i0 processi guided scheduling pragma omp parallel scheduleguided forint i0 processi q5 provide indepth explanation reductions parallel tasks openmp include importance typical use cases implementation techniques features help optimize parallel applications reductions reduction operations aggregate values computed independently across multiple threads single shared result safely efficiently openmps reduction clause automates aggregation sum product min max preventing race conditions example pragma omp parallel reductionsum forint i0 sum arrayi parallel tasks tasks represent dynamic units work executed asynchronously available threads handle irregular workloads recursive functions dynamic parallelism example recursive parallelism void quicksortint int left int right ifleft right int p partitiona left right pragma omp task quicksorta left p1 pragma omp task quicksorta p1 right use cases  reductions summation product calculations statistical analysis  parallel tasks recursive algorithms tree traversals divideandconquer strategies features significantly optimize parallel execution enhancing performance load balancing flexibility addressing diverse computational workloads effectively unit 3 distributed shared memory mechanism manages memory across multiple nodes makes interprocess communications transparent endusers applications think running shared memory dsm mechanism allowing user processes access shared data without using interprocess communications dsm every node memory provides memory read write services provides consistency protocols distributed shared memory dsm implements shared memory model distributed systems  physical shared memory nodes share virtual address space provided shared memory model data moves main memories different nodes types distributed shared memory 1 onchip memory  data present cpu portion chip  memory directly connected address lines  onchip memory dsm expensive complex 2 busbased multiprocessors  set parallel wires called bus acts connection cpu memory  accessing memory simultaneously multiple cpus prevented using algorithms  cache memory used reduce network traffic 3 ringbased multiprocessors  global centralized memory present ringbased dsm  nodes connected via token passing ring  ringbases dsm single address line divided shared area advantages distributed shared memory  simpler abstraction programmer need concern data movement address space easier implement rpc  easier portability access protocols used dsm allow natural transition sequential distributed systems dsm programs portable use common programming interface  locality data data moved large blocks ie data near current memory location fetched may needed future also fetched  ondemand data movement provided dsm eliminate data exchange phase  larger memory space provides large virtual memory space total memory size sum memory size nodes paging activities reduced  better performance dsm improve performance efficiency speeding access data  flexible communication environment join leave dsm system without affecting others need sender receiver existing  process migration simplified share address space one process easily moved different machine disadvantages distributed shared memory  accessibility data access slow dsm compare nondistributed  consistency programming done dsm systems programmers need maintain consistency  message passing dsm use asynchronous message passing efficient per message passing implementation  data redundancy dsm allows simultaneous access data consistency data redundancy common disadvantage  lower performance cpu gets slowed even cache memory aid situation message passing interface mpi message passing interface mpi standardized portable messagepassing system designed function wide variety parallel computing architectures mpi critical tool highperformance computing hpc commonly used programming parallel computers supports pointtopoint collective communication provides way handle complexities parallel environments optimizing data exchange processes mpi operation syntax mpi designed used c c fortran offering library functions subroutines fortran call programs functions allow send receive messages broadcast data perform reductions general syntax mpi operations c usually include mpih int mpifunctionnameparameter list syntax mpifunctionname represents mpi function called parameter list includes necessary data buffers data types communication tags mpi communicators mpi data types mpi supports variety data types correspond standard c c fortran data types facilitating transfer data machineindependent fashion data types include  primitive types mpiint integers mpifloat floatingpoint numbers mpichar characters  derived types allow complex structures sent received network created match userdefined structures code mpi also allows define data types particularly useful need send arrays sections arrays contiguous memory basic mpi operations mpi operations broadly categorized pointtopoint operations collective operations pointtopoint operations pointtopoint communication involves sending receiving messages two mpi processes operations include  mpisendvoid data int count mpidatatype datatype int destination int tag mpicomm comm sends message designated destination  mpirecvvoid data int count mpidatatype datatype int source int tag mpicomm comm mpistatus status receives message source mpisend mpirecv functions include parameters data buffer number elements data type communicator tag parameter used match sends receives collective operations collective operations involve processes communicator include functions data distribution collection synchronization  mpibcastvoid buffer int count mpidatatype datatype int root mpicomm comm broadcasts message root process processes communicator  mpireducevoid sendbuf void recvbuf int count mpidatatype datatype mpiop op int root mpicomm comm reduces data processes single result specified operation like sum max  mpiallreducevoid sendbuf void recvbuf int count mpidatatype datatype mpiop op mpicomm comm similar mpireduce result distributed back participating processes collective operations crucial implementing parallel algorithms require frequent communication data sharing processes conclusion mpi provides powerful framework developing parallel applications robust support variety data types operations abstracts many complexities associated direct hardware communication making easier scale applications across large distributed systems utilizing mpi developers optimize performance applications need handle large datasets perform complex computations parallel mpi message passing interface facilitates efficient scalable communication parallel computing environments supports range operations basic pointtopoint communication complex collective communications enabling processes cooperate share data effectively understanding operations key optimizing performance harnessing full power parallel processing systems processtoprocess communication processtoprocess communication mpi primarily handled pointtopoint communication methods methods basic form mpi communication involve sending receiving messages directly two mpi processes done synchronously asynchronously  synchronous communication eg mpissend sender waits receiver starts receiving data type communication ensures send operation completes corresponding receive initiated help avoid buffer overruns manage flow control  asynchronous communication eg mpiisend mpiirecv send receive operation returns immediately allowing process continue computation communication still progress improve utilization computational resources requires careful management data dependencies buffer states operations fundamental building complex parallel algorithms used extensively distributed computing handle data exchanges processes running different nodes processors measuring performance performance measurement mpi applications crucial optimizing communication computational efficiency key metrics include  latency time taken start actual data transfer includes time spent queues handling delays  bandwidth rate data transmitted communication started typically measured bytes per second  throughput total amount data transferred per unit time across entire system tools like mpiprofiler tau tuning analysis utilities used measure metrics tools help identify bottlenecks communication computation enabling developers refine code improve parallel efficiency collective mpi communication collective communication involves groups processes designed enhance data movement patterns typical parallel applications  broadcasting mpibcast single process sends data processes communicator  scattering mpiscatter distributes distinct pieces data single process different processes  gathering mpigather collects distinct pieces data processes single process  alltoall mpialltoall process sends data every process collective operations optimized performance various hardware architectures crucial tasks require coordinated action multiple processes computational fluid dynamics simulations matrix multiplication collective mpi data manipulations beyond simple data transfer mpi supports operations manipulate data across processes enhancing capability perform parallel computations directly  reductions mpireduce mpiallreduce perform operations like sum max logical andor data distributed across processes return result one processes  scan operations mpiscan perform prefix reduction cumulative operation data distributed across processes operations fundamental parallel algorithms requiring global data interdependencies numerical simulations statistical analysis result depends combined contributions many processes conclusion mpis rich set functionalities processtoprocess communication collective operations makes versatile tool developing highperformance parallel applications understanding leveraging capabilities developers design efficient faster computational models capable running largescale computing infrastructures optimizing mpi applications involves using operations efficiently also measuring tuning performance based applications specific needs underlying hardware architecture unit 4 communication computation overlap communication computation overlap context parallel computing especially mpi message passing interface refers technique overlapping data communication processes computation improve overall efficiency performance parallel applications approach essential highperformance computing hpc maximize resource utilization minimize idle times traditional parallel applications communication computation often performed sequentially process first send receive data communication completed proceed subsequent computations sequential handling lead significant inefficiencies particularly communication involves large data transfers network leading idle cpu cycles address modern mpi implementations parallel programming practices encourage overlap communication computation primarily achieved nonblocking communication operations mpiisend mpiirecv initiate send receive operation return control application without waiting operation complete allows process perform computations communication still underway benefits  improved performance overlapping communication computation total runtime parallel applications often significantly reduced less time spent waiting data transfers complete  increased efficiency utilizing cpugpu resources communication ongoing maximizes resource utilization reducing overall cost computation large scale systems  scalability overlapping communication computation help applications scale better large computing clusters mitigating impact increased communication overhead larger setups challenges  complexity programming implementing effective overlap requires careful control data dependencies program flow ensure computations proceed incomplete incorrect data  hardware network dependencies effectiveness overlapping strategies depend heavily underlying hardware network characteristics network bandwidth latency well presence hardware support nonblocking communication applications  scientific simulations applications like climate modeling molecular dynamics large volumes data need exchanged nodes simultaneously performing complex calculations  realtime data processing scenarios financial analytics media streaming data must processed nearrealtime continuously receiving new data inputs designing algorithms cleverly interleave data transfers computational tasks developers significantly enhance performance efficiency parallel applications mpi communicators mpi communicators fundamental mpi message passing interface programming model serving key component controlling scope context communication among processes mpi communicator encapsulates group mpi processes communicate identified communicator handle concept crucial managing complexity large parallel applications enabling structured secure communications key features  encapsulation communicators encapsulate communication space ensuring messages visible processes within communicator  security provide isolation level prevents accidental message passing unrelated parts program enhancing security fault isolation  flexibility developers define multiple communicators different parts application allowing modular design parallel library development types  mpicommworld default communicator includes mpi processes  subcommunicators users create custom communicators include subset processes targeted communication particularly useful multi component simulations different modules need communicate internally operations  barrier synchronizes processes communicator  broadcast sends data one process others communicator  reduce combines data processes single process using specified operation applications  domain decomposition simulations different regions simulated domain handled different subcommunicators  layered architecture multitier applications layer component might communicator internal communications mpi communicators powerful abstraction facilitate efficient organized communication patterns parallel computing environments essential building scalable robust mpi applications anatomy gpu anatomy gpu graphics processing unit reflects design handle massive parallelism required rendering graphics increasingly generalpurpose computing tasks gpgpu gpu composed several key components tailored efficiently process tasks concurrently 1 processing cores heart gpu array processing cores perform computations unlike cpus cores optimized sequential serial processing gpu contains thousands smaller efficient cores designed handling multiple tasks simultaneously 2 control units group cores controlled command processor scheduler dispatch tasks cores units manage execution instructions synchronize data flow cores memory 3 memory hierarchy gpus complex memory hierarchy includes register files l1l2 cache shared memory global memory design helps managing high throughput data required cores registers fastest form memory located directly processing cores shared memory allows threads within block share data quickly without accessing slower global memory global memory provides large slower storage area accessible cores 4 stream multiprocessors sms main computational units nvidia gpus example multiple cores grouped together sm contains cores scheduler registers shared memory working together execute multiple threads concurrently 5 texture render outputs specialized units handle tasks related texture mapping pixel rendering crucial graphic applications also useful certain computational tasks benefit data caching spatial locality 6 interconnects interconnection network within gpu links various components cores memory inputoutput systems allowing efficient data transfer across chip introduction gpu evolution evolution gpus marked significant milestones simple fixedfunction devices highly complex processors capable generalpurpose computing  early gpus primarily designed fixedfunction 2d later 3d graphics operations accelerated tasks like texture mapping zbuffering offered little programmability  programmable shaders introduction programmable shaders early 2000s marked significant shift allowing developers flexibility control graphics pipeline innovation paved way complex visual effects simulations directly gpu  gpgpu cuda mid2000s witnessed rise generalpurpose computing gpus facilitated nvidias cuda compute unified device architecture allowed gpu perform tasks traditionally handled cpus data analysis scientific computing leveraging parallel processing capabilities  architectural improvements years gpus seen vast improvements architecture including increased core counts larger efficient caches advanced features like ray tracing cores tensor cores optimized specific tasks like realtime ray tracing ai computations respectively modern gpu modern gpu complex piece technology optimized graphics rendering parallel computing tasks key features components include 1 unified architecture modern gpus feature unified architecture processing cores handle graphics computing tasks dynamically based workload demands 2 ray tracing ai cores recent gpu models include dedicated cores ray tracing ai tasks enhancing capabilities rendering realistic lighting shadows accelerating ai algorithms 3 increased memory bandwidth capacity support highresolution textures complex scenes modern gpus equipped highspeed gddr6 memory even hbm high bandwidth memory providing faster data access increased bandwidth 4 enhanced power efficiency advances fabrication technology allowed modern gpus become powerefficient crucial consumer devices largescale data centers 5 api framework support modern gpus supported broad ecosystem development tools apis like directx 12 vulkan cuda allow developers harness capabilities across various platforms applications 6 versatility beyond gaming professional graphics modern gpus extensively used areas like machine learning scientific simulations cryptocurrency mining demonstrating versatility importance contemporary computing tasks scheduling threads compute units modern gpus scheduling threads compute units critical aspect achieving high performance efficient utilization hardware resources gpus composed multiple compute units cus capable executing large number threads concurrently understanding threads scheduled executed across units key leveraging gpu architecture graphics rendering generalpurpose computing gpgpu thread warpworkgroup organization  threads smallest execution unit gpu thread executes part program known kernel threads grouped warps wavefronts depending terminology used nvidia amd respectively  warpsworkgroups group threads typically 32 nvidia 64 amd execute instruction time different data group threads scheduled single unit onto compute unit scheduling mechanism  simd execution compute units gpu operate simd single instruction multiple data basis cu set alus arithmetic logic units execute instruction different pieces data simultaneously ideal parallel nature graphics many computing tasks  dynamic scheduling modern gpus employ sophisticated dynamic scheduling hardware assign warps wavefronts compute units scheduler assesses readiness threads based data availability factors schedules keep compute units busy possible  latency hiding one strengths gpu architecture ability hide latencies typically associated memory accesses delays rapidly switching different warps workgroups one stalled eg waiting memory gpu maintain high utilization throughput factors affecting scheduling  resource availability amount shared memory registers available per compute unit limit number threads warps actively scheduled  dependency synchronization threads within warp execute lock step simplifies synchronization also means one thread waiting data dependency entire warp waits effective utilization gpu resources adept thread scheduling management warpsworkgroups essential optimizing performance applications ranging video rendering complex scientific simulations memory hierarchy gpu memory hierarchy gpu designed address high data throughput bandwidth requirements needed processing large blocks data parallel hierarchical structure optimizes access times maximizes efficiency positioning different types memory based speed size function 1 registers fastest type memory available gpu threads thread access registers store variables temporary data number registers limited allocation significantly affect performance kernel 2 shared memory located compute unit shared memory accessible threads within block group threads scheduled compute unit much faster global memory also limited size programmers often use shared memory facilitate efficient data exchange threads reduce global memory accesses 3 l1 l2 cache modern gpus include l1 cache accessible threads within compute unit larger l2 cache shared across multiple compute units caches store frequently accessed data reduce number slower global memory accesses 4 global memory largest slowest form memory gpu global memory accessible threads used store data fit faster memory types optimizing access patterns global memory eg coalescing accesses crucial maintaining high performance 5 constant texture memory specialized memory types optimized specific access patterns constant memory optimized scenarios threads access data texture memory provides hardware optimization texture fetching beneficial graphics certain gpgpu applications memory hierarchy gpus critical element supports massive parallel processing capabilities understanding optimizing memory usage according hierarchy significantly enhance performance gpuaccelerated applications making efficient memory management key skill gpu programming opencl execution model opencl open computing language framework writing programs execute across heterogeneous platforms consisting central processing units cpus graphics processing units gpus processors opencl provides standard interface parallel computing using taskbased databased parallelism opencl execution model describes code written opencl executes computing devices designed give developers control tasks divided executed across wide array hardware 1 platform devices opencl platform host plus collection devices gpus cpus etc controlled single opencl implementation device divided compute units processing elements 2 context environment within kernels execute domain synchronization memory management defined context includes devices kernels functions executed opencl devices memory objects 3 command queues context multiple command queues commands execute kernels readwrite memory queued configured execute inorder outoforder respect commands queue 4 kernels workgroups fundamental unit executable code opencl kernel kernel executed runs across ndimensional range work items organized workgroups workgroup executed single compute unit work item maps processing element within unit 5 synchronization opencl provides synchronization mechanisms within workgroups crucial managing dependencies ensuring correct computation opencl execution model flexible scalable allowing programs efficiently exploit parallel computational elements cpus gpus opencl memory model opencl memory model defines several types memory different scopes lifetimes caching behaviors allowing optimized data management across different types computing devices 1 global memory accessible work items workgroups global memory largest also slowest form memory available opencl ideal storing data needs accessed many work items 2 local memory local workgroup scratchpad memory shared work items within workgroup much faster global memory useful sharing data work items performing cooperative tasks within workgroup 3 private memory work item private memory accessible work items typically fastest form memory used variables local work item 4 constant memory region global memory remains constant execution kernel cached device faster access best used data change accessed work items understanding utilizing memory types effectively greatly influence performance opencl program developer needs carefully design kernel memory usage pattern minimize reliance slower memory types maximize computational throughput leveraging execution memory models opencl enables programmers develop applications run various hardware architectures achieving high levels parallelism performance makes opencl powerful tool applications ranging scientific simulations machine learning unit 5 programming opencl programming opencl involves combination host code typically written c c device code kernels written language similar c process generally follows steps 1 platform device selection identify select available opencl platforms devices cpus gpus etc code run 2 context queue creation create opencl context selected device command queues queuing kernel execution memory commands 3 memory management allocate memory host create memory buffers device transfer data device memory executing kernel 4 kernel programming write opencl kernel defines computation work item 5 building enqueueing kernels compile kernel code runtime set kernel arguments enqueue kernel execution device 6 execution data retrieval execute kernels retrieve results back host reading device buffers sum arbitrary long vectors sum two large vectors opencl write kernel adds elements vector parallel kernel void vectoraddglobal const float global const float b global float c const int n int id getglobalid0 id n cid aid bid kernel retrieves global index checks within bounds vectors performs elementwise addition host program needs handle data setup kernel execution data retrieval dot product opencl basic dot product opencl computed thread calculating product corresponding vector elements summing products get final result  straightforward implementation kernel void dotproductglobal const float global const float b global float c const int n int id getglobalid0 id n cid aid bid get final dot product sum results c device retrieve c host sum dot product opencl using local memory using local memory compute dot product optimizes operation reducing global memory bandwidth usage kernel computes partial sums parallel combines kernel void dotproductlocalglobal float global float b global float c local float temp const int n int id getglobalid0 int localid getlocalid0 templocalid aid bid barrierclklocalmemfence localid 0 float sum 0 int 0 getlocalsize0 sum tempi atomicaddc0 sum kernel uses local memory storing intermediate results reduces within workgroup atomic operation used safely accumulate workgroup results global sum naive matrix multiplication opencl naive matrix multiplication directly translates standard triplenested loop matrix multiplication algorithm opencl kernel kernel void matrixmultnaiveglobal float global float b global float c int width int row getglobalid0 int col getglobalid1 float sum 0 int k 0 k width k sum arow width k bk width col crow width col sum tiled matrix multiplication opencl tiled blocked matrix multiplication improves upon naive approach dividing matrices smaller blocks tiles method takes advantage local memory reduce frequency global memory access kernel void matrixmulttiledglobal float global float b global float c int width int tilesize local float asub1616 local float bsub1616 int bx getgroupid0 int getgroupid1 int tx getlocalid0 int ty getlocalid1 int row tilesize ty int col bx tilesize tx float sum 0 int 0 widthtilesize asubtytx arow width tilesize tx bsubtytx bm tilesize ty width col barrierclklocalmemfence int k 0 k tilesize k sum asubtyk bsubktx barrierclklocalmemfence crow width col sum kernel loads tile tile b local memory thread calculates element block c using tiles local memory minimizes number accesses global memory significantly improves performance\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from rouge import Rouge\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_pdf(file_path = \"../Prroject -Text Summarization/PC.pdf\"):\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    doc.close()\n",
        "    print(\"Extracted text length:\", len(text))  # Debugging line\n",
        "    return text\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def summarize2(text, max_words):\n",
        "    sentences = sent_tokenize(text)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
        "    scores = np.sum(cosine_similarity(sentence_vectors[0:1], sentence_vectors), axis=1)\n",
        "\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    summary_sentences = []\n",
        "    word_count = 0\n",
        "\n",
        "    for index in sorted_indices:\n",
        "        sentence_word_count = len(word_tokenize(sentences[index]))\n",
        "        if word_count + sentence_word_count > max_words:\n",
        "            break\n",
        "        summary_sentences.append(sentences[index])\n",
        "        word_count += sentence_word_count\n",
        "\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "\n",
        "file_path = \"../Prroject -Text Summarization/PC.pdf\"\n",
        "document = read_pdf(file_path)\n",
        "processed_text = preprocess(document)\n",
        "Adsummary = summarize2(processed_text, max_words=45505)\n",
        "print(\"Summary:\", Adsummary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe-dca1IACjR",
        "outputId": "6cbb92e0-2265-4c59-cc24-83f433ec40f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Word Count: 18858\n",
            "Summary Word Count: 11084\n",
            "Compression Ratio: 0.5877611623714074\n"
          ]
        }
      ],
      "source": [
        "def evaluate_PDF(original_text, generated_summary):\n",
        "    original_count = len(word_tokenize(original_text))\n",
        "    summary_count = len(word_tokenize(Adsummary))\n",
        "    compression_ratio = summary_count / original_count\n",
        "    return original_count, summary_count, compression_ratio\n",
        "\n",
        "original_count, summary_count, compression_ratio = evaluate_PDF(document, summary)\n",
        "print(\"Original Word Count:\", original_count)\n",
        "print(\"Summary Word Count:\", summary_count)\n",
        "print(\"Compression Ratio:\", compression_ratio)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hybrid Techniques to implement the Text summarization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " hybrid text summarization involves combining machine learning and optimization techniques. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\loq\\anaconda3\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in c:\\users\\loq\\anaconda3\\lib\\site-packages (4.3.3)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\loq\\anaconda3\\lib\\site-packages (1.5.1)\n",
            "Requirement already satisfied: click in c:\\users\\loq\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\loq\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in c:\\users\\loq\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\loq\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk gensim scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\loq\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install python-docx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text Word Count: 19158\n",
            "Generated Summary Word Count: 12229\n",
            "Compression Ratio: 0.6383234158054076\n",
            "Word document saved as summarized_output.docx\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from docx import Document\n",
        "from rouge import Rouge\n",
        "\n",
        "\n",
        "def read_text_from_pdf(pdf_path):\n",
        "    from PyPDF2 import PdfReader\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = ''\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    return [word.lower() for word in nltk.word_tokenize(sentence) if word.isalnum()]\n",
        "\n",
        "def create_sentence_vectors(sentences, dictionary, tfidf):\n",
        "    sentence_vectors = []\n",
        "    for sentence in sentences:\n",
        "        words = preprocess_sentence(sentence)\n",
        "        bow = dictionary.doc2bow(words)\n",
        "        dense_vector = np.zeros(len(dictionary), dtype=float)\n",
        "        for idx, score in tfidf[bow]:\n",
        "            dense_vector[idx] = score\n",
        "        sentence_vectors.append(dense_vector)\n",
        "    return sentence_vectors\n",
        "\n",
        "def cluster_sentences(sentence_vectors, num_clusters):\n",
        "    kmeans = KMeans(n_clusters=num_clusters)\n",
        "    kmeans.fit(sentence_vectors)\n",
        "    return kmeans.labels_\n",
        "\n",
        "def summarize_text(text, num_clusters=520):\n",
        "    sentences = sent_tokenize(text)\n",
        "    processed_sents = [preprocess_sentence(sent) for sent in sentences]\n",
        "    \n",
        "    dictionary = Dictionary(processed_sents)\n",
        "    bow_corpus = [dictionary.doc2bow(sent) for sent in processed_sents]\n",
        "    tfidf_model = TfidfModel(bow_corpus)\n",
        "    \n",
        "    sentence_vectors = create_sentence_vectors(sentences, dictionary, tfidf_model)\n",
        "    clusters = cluster_sentences(sentence_vectors, num_clusters)\n",
        "    \n",
        "    cluster_to_sentence = {}\n",
        "    for i, cluster_id in enumerate(clusters):\n",
        "        if cluster_id not in cluster_to_sentence:\n",
        "            cluster_to_sentence[cluster_id] = i\n",
        "        else:\n",
        "            existing_idx = cluster_to_sentence[cluster_id]\n",
        "            if sum(sentence_vectors[i]) > sum(sentence_vectors[existing_idx]):\n",
        "                cluster_to_sentence[cluster_id] = i\n",
        "    \n",
        "    selected_sentences = sorted(cluster_to_sentence.values())\n",
        "    summary = ' '.join([sentences[idx] for idx in selected_sentences])\n",
        "    \n",
        "    return summary\n",
        "\n",
        "def evaluate_summary(hypothesis, reference):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(hypothesis, reference)\n",
        "    return scores\n",
        "\n",
        "def word_count(text):\n",
        "    return len(word_tokenize(text))\n",
        "\n",
        "def save_to_word(document_text, filename):\n",
        "    doc = Document()\n",
        "    doc.add_paragraph(document_text)\n",
        "    doc.save(filename)\n",
        "    print(f'Word document saved as {filename}')\n",
        "    \n",
        "    \n",
        "pdf_path = '../Prroject -Text Summarization/PC.pdf'\n",
        "text = read_text_from_pdf(pdf_path)\n",
        "generated_summary = summarize_text(text)\n",
        "\n",
        "# Calculate and print word counts and compression ratio\n",
        "original_word_count = word_count(text)\n",
        "summary_word_count = word_count(generated_summary)\n",
        "compression_ratio = summary_word_count / original_word_count\n",
        "\n",
        "print(\"Original Text Word Count:\", original_word_count)\n",
        "print(\"Generated Summary Word Count:\", summary_word_count)\n",
        "print(\"Compression Ratio:\", compression_ratio)\n",
        "\n",
        "save_to_word(generated_summary, 'summarized_output.docx')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
